{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNZxvvaXxQEXslowCpLlMLW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Bp7BdqKtbsOP"},"outputs":[],"source":[]},{"cell_type":"code","source":["#step1# dataset code\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# Create a directory to store the generated images\n","dataset_dir = 'graph_dataset'\n","os.makedirs(dataset_dir, exist_ok=True)\n","\n","# Define parameters\n","num_samples = 2000  # Number of samples\n","num_points_range = (2, 5)  # Range for number of annotation points\n","\n","# Generate dataset\n","data = []\n","for i in range(num_samples):\n","    # Generate random data points for a straight line graph\n","    x_values = np.arange(0, 10, 0.5)\n","    m, c = np.random.uniform(0.5, 2), np.random.uniform(5, 10)\n","    y_values = m * x_values + c\n","\n","    # Select random number of annotation points\n","    num_points = np.random.randint(*num_points_range)\n","\n","    # Randomly select indices for annotation points\n","    annotation_indices = np.random.choice(len(x_values), size=num_points, replace=False)\n","\n","    # Generate annotations with coordinates (without values)\n","    annotations = [f'({x_values[idx]:.1f}, {y_values[idx]:.1f})' for idx in annotation_indices]\n","\n","    # Generate explanation including annotations\n","    explanation = f\"This is a straight line graph with random annotation points:\\n\"\n","    explanation += f\"Equation: y = {m:.2f}x + {c:.2f}\\n\"\n","    explanation += \"Annotations:\\n\"\n","    for annotation in annotations:\n","        explanation += f\"- {annotation}\\n\"\n","\n","    # Plot the graph\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(x_values, y_values, linestyle='-', color='b')\n","    for annotation in annotations:\n","        if annotation.strip():  # Check if annotation has values\n","            x, y = map(float, annotation.split('(')[1].split(')')[0].split(', '))\n","            plt.scatter(x, y, color='r')\n","            plt.annotate(annotation, (x, y), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n","    plt.title(f'Straight Line Graph {i+1}')\n","    plt.xlabel('X-axis')\n","    plt.ylabel('Y-axis')\n","    plt.grid(True)\n","    plt.tight_layout()\n","\n","    # Save the graph as an image\n","    graph_image_path = os.path.join(dataset_dir, f'straight_line_graph_{i+1}.png')\n","    plt.savefig(graph_image_path)\n","    plt.close()\n","\n","    # Add data to dataset\n","    data.append({'image_path': graph_image_path, 'annotations': annotations, 'explanation': explanation})\n","\n","# Convert dataset to DataFrame and save as CSV\n","df = pd.DataFrame(data)\n","df.to_csv(os.path.join(dataset_dir, 'graph_dataset.csv'), index=False)\n","\n","print(\"Dataset created successfully\")"],"metadata":{"id":"s8hXBBLMH1D1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709926187678,"user_tz":-330,"elapsed":674389,"user":{"displayName":"Aathif Aslam","userId":"07585414347874006018"}},"outputId":"75221bbb-8564-49c7-81ac-4f3a18dfffde"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset created successfully\n"]}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Load the CSV file containing annotations and image paths\n","csv_file_path = '/content/graph_dataset/graph_dataset.csv'\n","if not os.path.exists(csv_file_path):\n","    raise FileNotFoundError(f\"CSV file '{csv_file_path}' not found.\")\n","\n","dataset = pd.read_csv(csv_file_path)\n","\n","# Preprocess images (resize, normalize, etc.)\n","image_directory = 'graph_dataset'\n","if not os.path.exists(image_directory):\n","    raise FileNotFoundError(f\"Image directory '{image_directory}' not found.\")\n","\n","images = []\n","for path in dataset['image_path']:\n","    img_path = os.path.join(image_directory, path)\n","    if not os.path.exists(img_path):\n","        print(f\"Image not found: {img_path}\")\n","        continue\n","    img = plt.imread(img_path)\n","    # Perform preprocessing steps (e.g., resizing to (224, 224), normalization)\n","    img = preprocess_image(img)\n","    images.append(img)\n","images = np.array(images)\n","vocab_size=10000\n","# Tokenize text explanations\n","text_explanations = dataset['explanation'].values\n","tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n","tokenizer.fit_on_texts(text_explanations)\n","sequences = tokenizer.texts_to_sequences(text_explanations)\n","max_seq_length = max(len(seq) for seq in sequences)\n","padded_sequences = pad_sequences(sequences, maxlen=max_seq_length)\n","\n","# Split data into features (images) and labels (explanations)\n","X_images = images\n","X_text = padded_sequences\n","y = dataset['Label']  # Assuming 'Label' column contains the class labels\n","\n","# Split the data into training and validation sets\n","X_images_train, X_images_val, X_text_train, X_text_val, y_train, y_val = train_test_split(\n","    X_images, X_text, y, test_size=0.2, random_state=42)\n"],"metadata":{"id":"GEewlzqpcdb5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FfRaDSAOiG-W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 3: Model Selection\n","# Define the model architecture for image and text analysis\n","\n","# Define image input shape\n","image_input_shape = (224, 224, 3)  # Example: ResNet50 input shape\n","\n","# Define text input parameters\n","vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size including OOV token\n","embedding_dim = 100  # Dimensionality of word embeddings\n","\n","# Define model architecture\n","image_input = Input(shape=image_input_shape)\n","text_input = Input(shape=(max_seq_length,))\n","# Example CNN model for image analysis\n","image_model = build_cnn_model(image_input)\n","# Example LSTM model for text analysis\n","text_model = build_lstm_model(text_input, vocab_size, embedding_dim)\n","\n","# Combine image and text features\n","combined_features = concatenate([image_model.output, text_model.output])\n","# Example Dense layers for classification\n","output = Dense(num_classes, activation='softmax')(combined_features)\n","\n","# Create the final model\n","model = Model(inputs=[image_input, text_input], outputs=output)\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"],"metadata":{"id":"_jZPIjSrcQ3_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 4: Model Training\n","# Train the model using the prepared dataset\n","def train_model(model, train_data, val_data, epochs=10, batch_size=32):\n","    # Separate features (image paths) and labels (explanations) from the dataset\n","    image_paths = train_data['Image_Path'].values\n","    explanations = train_data['Explanation'].values\n","\n","    # Split the dataset into training and validation sets\n","    train_image_paths, val_image_paths, train_explanations, val_explanations = train_test_split(\n","        image_paths, explanations, test_size=0.2, random_state=42)\n","\n","    # Example of image data generator for loading and augmenting images\n","    train_image_generator = ImageDataGenerator(rescale=1./255)\n","    val_image_generator = ImageDataGenerator(rescale=1./255)\n","\n","    train_image_data = train_image_generator.flow_from_dataframe(\n","        dataframe=train_data,\n","        x_col='Image_Path',\n","        y_col='Explanation',\n","        directory=image_directory,\n","        target_size=(224, 224),\n","        batch_size=batch_size,\n","        class_mode='categorical',  # or 'raw' if you're using custom data generator\n","        shuffle=True\n","    )\n","\n","    val_image_data = val_image_generator.flow_from_dataframe(\n","        dataframe=val_data,\n","        x_col='Image_Path',\n","        y_col='Explanation',\n","        directory=image_directory,\n","        target_size=(224, 224),\n","        batch_size=batch_size,\n","        class_mode='categorical',  # or 'raw' if you're using custom data generator\n","        shuffle=False\n","    )\n","\n","    # Train the model\n","    history = model.fit(train_image_data, epochs=epochs, validation_data=val_image_data)\n","\n","    return history"],"metadata":{"id":"20L8CojIcBQP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 5: Evaluation\n","# Evaluate the model performance on a validation set\n","def evaluate_model(model, val_data):\n","    # Load the trained model\n","    trained_model = model  # Example: trained_model = load_model('best_model.h5')\n","\n","    # Separate features (image paths) and labels (explanations) from the validation dataset\n","    image_paths = val_data['Image_Path'].values\n","    explanations = val_data['Explanation'].values\n","\n","    # Example of image data generator for loading images\n","    val_image_generator = ImageDataGenerator(rescale=1./255)\n","\n","    val_image_data = val_image_generator.flow_from_dataframe(\n","        dataframe=val_data,\n","        x_col='Image_Path',\n","        y_col='Explanation',\n","        directory=image_directory,\n","        target_size=(224, 224),\n","        batch_size=1,  # Use batch size of 1 for evaluation\n","        class_mode='categorical',  # or 'raw' if you're using custom data generator\n","        shuffle=False  # Ensure the order of predictions matches the order of images\n","    )\n","\n","    # Evaluate the model\n","    loss, accuracy = trained_model.evaluate(val_image_data)\n","    print(f'Validation Loss: {loss}, Validation Accuracy: {accuracy}')\n"],"metadata":{"id":"GOJoR_CbcGdd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 6: Fine-tuning and Optimization\n","# Fine-tune the model architecture, hyperparameters, and training process as needed.\n","\n","# Example: Fine-tuning hyperparameters\n","# Define hyperparameters\n","epochs = 10\n","batch_size = 32\n","\n","# Train the model with fine-tuned hyperparameters\n","history = model.fit([X_images_train, X_text_train], y_train,\n","                    validation_data=([X_images_val, X_text_val], y_val),\n","                    epochs=epochs, batch_size=batch_size)\n"],"metadata":{"id":"ru56prTedULR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 7: Testing (Validation)\n","# Evaluate the trained model on the validation set to estimate its performance.\n","\n","val_loss, val_accuracy = model.evaluate([X_images_val, X_text_val], y_val)\n","print(f'Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')\n","\n"],"metadata":{"id":"QpkJyA8vdq_u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 8: Deployment\n","# Deploy the trained model for inference, integrate it into an application, or serve it through an API.\n","\n","# Example: Save the trained model for deployment\n","joblib.dump(model, 'image_model1.h5')\n","\n","\n","# Example: Load the trained model for inference\n","deployed_model = load_model('/content/image_model1.h5')\n","\n","# Example: Perform inference on new data\n","new_data_images = preprocess_new_images(new_images)  # Preprocess new images\n","new_data_text = preprocess_new_text(new_text)  # Preprocess new text explanations\n","predictions = deployed_model.predict([new_data_images, new_data_text])\n"],"metadata":{"id":"hPT5DQ3ZdVQW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os    #new code\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Reshape, concatenate, Dropout, add, GlobalAveragePooling2D, RepeatVector\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.utils import plot_model\n","\n","# Load the dataset\n","csv_file_path = '/content/graph_dataset/graph_dataset.csv'\n","dataset = pd.read_csv(csv_file_path)\n","\n","# Define image input shape and maximum sequence length\n","image_input_shape = (224, 224, 3)\n","max_length = 100\n","\n","# Define vocabulary size (assuming you have already defined this)\n","vocab_size = ...\n","\n","# Define the inputs\n","input1 = Input(shape=image_input_shape)\n","input2 = Input(shape=(max_length,))\n","\n","# Extract features from the image\n","img_features = Dense(256, activation='relu')(input1)\n","img_features_reshaped = GlobalAveragePooling2D()(img_features)\n","img_features_reshaped = Reshape((1, 256))(img_features_reshaped)\n","\n","# Process text input\n","sentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\n","sentence_features = LSTM(256)(sentence_features)\n","sentence_features = RepeatVector(1)(sentence_features)  # Repeat the features to match the shape of img_features_reshaped\n","\n","# Combine image and text features\n","merged = concatenate([img_features_reshaped, sentence_features], axis=1)\n","x = Dropout(0.5)(merged)\n","x = Dense(128, activation='relu')(x)\n","x = Dropout(0.5)(x)\n","output = Dense(vocab_size, activation='softmax')(x)\n","\n","# Create and compile the model\n","caption_model = Model(inputs=[input1, input2], outputs=output)\n","caption_model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","# Plot the model architecture\n","plot_model(caption_model, to_file='caption_model.png', show_shapes=True)\n","\n","# Display model summary\n","caption_model.summary()\n","\n","# Define data generators\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","val_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_dataframe(\n","    dataframe=dataset,\n","    directory='/content/graph_dataset/',  # Path to the directory containing images\n","    x_col='Image_Path',  # Column name containing image file names\n","    y_col='Annotations',  # Column name containing text annotations\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='raw',  # Use 'raw' since annotations are not categorical labels\n","    shuffle=True)\n","\n","# Split the dataset into training and validation sets\n","train_df, val_df = train_test_split(dataset, test_size=0.2, random_state=42)\n","\n","# Define the validation generator\n","validation_generator = val_datagen.flow_from_dataframe(\n","    dataframe=val_df,\n","    directory='/content/graph_dataset/',  # Path to the directory containing images\n","    x_col='Image_Path',  # Column name containing image file names\n","    y_col='Annotations',  # Column name containing text annotations\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='raw',  # Use 'raw' since annotations are not categorical labels\n","    shuffle=False)  # No need to shuffle validation data\n","\n","# Define callbacks\n","model_checkpoint = ModelCheckpoint('caption_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n","\n","# Train the model\n","history = caption_model.fit(train_generator,\n","                            epochs=10,\n","                            validation_data=validation_generator,\n","                            callbacks=[model_checkpoint, early_stopping, reduce_lr])\n","\n","# Plot training history\n","plt.plot(history.history['loss'], label='train_loss')\n","plt.plot(history.history['val_loss'], label='val_loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"gMAXL3CUiIYu"},"execution_count":null,"outputs":[]}]}